{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 6 # 22 Tiny LLAMA\n",
    "n_heads = 6 # 32 Tiny LLAMA\n",
    "d_model = 768 # 2048 Tiny LLAMA\n",
    "intermediate_dim = d_model * 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### MHA\n",
    "<img src=\"https://data-science-blog.com/wp-content/uploads/2022/01/mha_img_original.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random data \n",
    "sequence_length = 10 # number of tokens\n",
    "batch_size = 5\n",
    "input_data = torch.rand((batch_size, sequence_length, d_model)) # [batch_size, sequence_length, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.q = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.k = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.v = nn.Linear(embed_dim, hidden_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):    \n",
    "        dim_k = q.size(-1)\n",
    "        scores = torch.bmm(q, k.transpose(1, 2)) / sqrt(dim_k) # k.T = [batch_size, sequence_length, embed_dim]  -> [batch_size, embed_dim, sequence_length]\n",
    "        if mask is not None:\n",
    "            scores = torch.masked_fill(scores, mask == 0, -torch.inf)\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        return torch.bmm(weights, v)\n",
    "    def forward(self, hidden_state, mask=None):\n",
    "        output = self.scaled_dot_product_attention(\n",
    "            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state), mask=mask)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 128])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = AttentionHead(d_model, d_model//n_heads)\n",
    "attn(input_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, n_heads, hidden_dim):\n",
    "        super(MHA, self).__init__()\n",
    "        embed_dim = hidden_dim\n",
    "        head_dim = hidden_dim // n_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(embed_dim, head_dim) for _ in range(n_heads)]\n",
    "            )\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "    def forward(self, hidden_state):\n",
    "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1 )\n",
    "        return self.out_proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 768])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MHA(n_heads, d_model)\n",
    "mha(input_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 768])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMAMLP(nn.Module):\n",
    "    def __init__(self, hidden_dim, intermediate_dim):\n",
    "        super(LLaMAMLP, self).__init__()\n",
    "        hidden_dim_factor = 4\n",
    "        self.linear_1 = nn.Linear(hidden_dim, intermediate_dim)\n",
    "        self.linear_2 = nn.Linear(hidden_dim, intermediate_dim)\n",
    "        self.activation_fn = nn.SiLU()\n",
    "        self.out_proj = nn.Linear(intermediate_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        x_fc_1 = self.linear_1(hidden_state)\n",
    "        x_fc_2 = self.linear_2(hidden_state)\n",
    "        x = self.activation_fn(x_fc_1) * x_fc_2\n",
    "        return self.out_proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp=LLaMAMLP(d_model, intermediate_dim)\n",
    "mlp(input_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_heads, hidden_dim, intermediate_dim):\n",
    "        super(Block, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.mha = MHA(n_heads=n_heads, hidden_dim=hidden_dim)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.mlp = LLaMAMLP(hidden_dim, intermediate_dim)\n",
    "\n",
    "    def forward(self, hidden_state, mask = None):\n",
    "        x = self.mha(hidden_state)\n",
    "        x = self.layer_norm(hidden_state) + x\n",
    "        x_fc = self.mlp(x)\n",
    "        x += x_fc\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 768])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = Block(n_heads, d_model, intermediate_dim)\n",
    "block(input_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class babyLLaMA(nn.Module):\n",
    "    def __init__(self, max_seq_len, vocab_size, n_layers, n_heads, hidden_dim, intermediate_dim):\n",
    "        super(babyLLaMA, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.pos = nn.Embedding(max_seq_len, hidden_dim)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(n_heads, hidden_dim, intermediate_dim) for _ in range(n_layers)]\n",
    "            )\n",
    "        self.out_proj = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        emb = self.emb(hidden_state)\n",
    "        seq_len = hidden_state.size(1)\n",
    "        positions = torch.arange(seq_len, dtype=torch.long).unsqueeze(0)\n",
    "        pos = self.pos(positions)\n",
    "        x = emb + pos\n",
    "        for b in self.blocks:\n",
    "            x = b(x)\n",
    "        \n",
    "        x = self.out_proj(x)\n",
    "        return F.softmax(x, dim=-1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = babyLLaMA(d_model, 32000, n_layers, n_heads, d_model, intermediate_dim)\n",
    "input_ids = torch.randint(1, 32000, (batch_size, sequence_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 32000])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(input_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
